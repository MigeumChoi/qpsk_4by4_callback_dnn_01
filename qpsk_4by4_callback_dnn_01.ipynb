{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN2MZJVIZxq9h0P6bBgK7bC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87sF1jtkb7Rl","executionInfo":{"status":"ok","timestamp":1694405661265,"user_tz":-540,"elapsed":11951,"user":{"displayName":"최미금","userId":"03270121767541003919"}},"outputId":"1149c494-4af2-46f3-9da3-954c9c02774c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.4368\n","Epoch 1: val_loss improved from inf to 2.97665, saving model to hl5_0100.h5\n","1/1 [==============================] - 1s 1s/step - loss: 3.4368 - val_loss: 2.9767\n","Epoch 2/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.4228\n","Epoch 2: val_loss improved from 2.97665 to 2.96625, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 3.4228 - val_loss: 2.9662\n","Epoch 3/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.4100\n","Epoch 3: val_loss improved from 2.96625 to 2.95506, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 3.4100 - val_loss: 2.9551\n","Epoch 4/3000\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]},{"output_type":"stream","name":"stdout","text":["\r1/1 [==============================] - ETA: 0s - loss: 3.3960\n","Epoch 4: val_loss improved from 2.95506 to 2.94276, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 71ms/step - loss: 3.3960 - val_loss: 2.9428\n","Epoch 5/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.3803\n","Epoch 5: val_loss improved from 2.94276 to 2.92912, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 3.3803 - val_loss: 2.9291\n","Epoch 6/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.3627\n","Epoch 6: val_loss improved from 2.92912 to 2.91389, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 3.3627 - val_loss: 2.9139\n","Epoch 7/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.3427\n","Epoch 7: val_loss improved from 2.91389 to 2.89687, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 3.3427 - val_loss: 2.8969\n","Epoch 8/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.3201\n","Epoch 8: val_loss improved from 2.89687 to 2.87797, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 3.3201 - val_loss: 2.8780\n","Epoch 9/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.2945\n","Epoch 9: val_loss improved from 2.87797 to 2.85695, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 3.2945 - val_loss: 2.8569\n","Epoch 10/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.2656\n","Epoch 10: val_loss improved from 2.85695 to 2.83351, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 3.2656 - val_loss: 2.8335\n","Epoch 11/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.2330\n","Epoch 11: val_loss improved from 2.83351 to 2.80749, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 3.2330 - val_loss: 2.8075\n","Epoch 12/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.1962\n","Epoch 12: val_loss improved from 2.80749 to 2.77861, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 75ms/step - loss: 3.1962 - val_loss: 2.7786\n","Epoch 13/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.1547\n","Epoch 13: val_loss improved from 2.77861 to 2.74631, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 3.1547 - val_loss: 2.7463\n","Epoch 14/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.1078\n","Epoch 14: val_loss improved from 2.74631 to 2.71025, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 77ms/step - loss: 3.1078 - val_loss: 2.7103\n","Epoch 15/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.0547\n","Epoch 15: val_loss improved from 2.71025 to 2.66970, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 3.0547 - val_loss: 2.6697\n","Epoch 16/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.9947\n","Epoch 16: val_loss improved from 2.66970 to 2.62424, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 2.9947 - val_loss: 2.6242\n","Epoch 17/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.9269\n","Epoch 17: val_loss improved from 2.62424 to 2.57329, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 69ms/step - loss: 2.9269 - val_loss: 2.5733\n","Epoch 18/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.8503\n","Epoch 18: val_loss improved from 2.57329 to 2.51611, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 2.8503 - val_loss: 2.5161\n","Epoch 19/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.7640\n","Epoch 19: val_loss improved from 2.51611 to 2.45220, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 78ms/step - loss: 2.7640 - val_loss: 2.4522\n","Epoch 20/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.6669\n","Epoch 20: val_loss improved from 2.45220 to 2.38084, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 95ms/step - loss: 2.6669 - val_loss: 2.3808\n","Epoch 21/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.5578\n","Epoch 21: val_loss improved from 2.38084 to 2.30115, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 2.5578 - val_loss: 2.3011\n","Epoch 22/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.4354\n","Epoch 22: val_loss improved from 2.30115 to 2.21199, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 2.4354 - val_loss: 2.2120\n","Epoch 23/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.2987\n","Epoch 23: val_loss improved from 2.21199 to 2.11233, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 68ms/step - loss: 2.2987 - val_loss: 2.1123\n","Epoch 24/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.1465\n","Epoch 24: val_loss improved from 2.11233 to 2.00141, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 65ms/step - loss: 2.1465 - val_loss: 2.0014\n","Epoch 25/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.9778\n","Epoch 25: val_loss improved from 2.00141 to 1.87848, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 1.9778 - val_loss: 1.8785\n","Epoch 26/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.7916\n","Epoch 26: val_loss improved from 1.87848 to 1.74228, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 1.7916 - val_loss: 1.7423\n","Epoch 27/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.5880\n","Epoch 27: val_loss improved from 1.74228 to 1.59128, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 1.5880 - val_loss: 1.5913\n","Epoch 28/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.3665\n","Epoch 28: val_loss improved from 1.59128 to 1.42420, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 1.3665 - val_loss: 1.4242\n","Epoch 29/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.1283\n","Epoch 29: val_loss improved from 1.42420 to 1.23978, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 1.1283 - val_loss: 1.2398\n","Epoch 30/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.8763\n","Epoch 30: val_loss improved from 1.23978 to 1.03793, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 76ms/step - loss: 0.8763 - val_loss: 1.0379\n","Epoch 31/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.6186\n","Epoch 31: val_loss improved from 1.03793 to 0.82524, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 70ms/step - loss: 0.6186 - val_loss: 0.8252\n","Epoch 32/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3780\n","Epoch 32: val_loss improved from 0.82524 to 0.61610, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 72ms/step - loss: 0.3780 - val_loss: 0.6161\n","Epoch 33/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1914\n","Epoch 33: val_loss improved from 0.61610 to 0.43262, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 73ms/step - loss: 0.1914 - val_loss: 0.4326\n","Epoch 34/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1039\n","Epoch 34: val_loss improved from 0.43262 to 0.29773, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 101ms/step - loss: 0.1039 - val_loss: 0.2977\n","Epoch 35/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1457\n","Epoch 35: val_loss improved from 0.29773 to 0.22245, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 129ms/step - loss: 0.1457 - val_loss: 0.2224\n","Epoch 36/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2914\n","Epoch 36: val_loss improved from 0.22245 to 0.19446, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 88ms/step - loss: 0.2914 - val_loss: 0.1945\n","Epoch 37/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.4350\n","Epoch 37: val_loss improved from 0.19446 to 0.18709, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 99ms/step - loss: 0.4350 - val_loss: 0.1871\n","Epoch 38/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.4881\n","Epoch 38: val_loss improved from 0.18709 to 0.18577, saving model to hl5_0100.h5\n","1/1 [==============================] - 0s 126ms/step - loss: 0.4881 - val_loss: 0.1858\n","Epoch 39/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.4615\n","Epoch 39: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 78ms/step - loss: 0.4615 - val_loss: 0.1910\n","Epoch 40/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.3756\n","Epoch 40: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 49ms/step - loss: 0.3756 - val_loss: 0.2075\n","Epoch 41/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.2647\n","Epoch 41: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 50ms/step - loss: 0.2647 - val_loss: 0.2386\n","Epoch 42/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1619\n","Epoch 42: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 51ms/step - loss: 0.1619 - val_loss: 0.2837\n","Epoch 43/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0869\n","Epoch 43: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0869 - val_loss: 0.3384\n","Epoch 44/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0453\n","Epoch 44: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0453 - val_loss: 0.3970\n","Epoch 45/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0320\n","Epoch 45: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0320 - val_loss: 0.4540\n","Epoch 46/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0384\n","Epoch 46: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0384 - val_loss: 0.5051\n","Epoch 47/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0552\n","Epoch 47: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 63ms/step - loss: 0.0552 - val_loss: 0.5474\n","Epoch 48/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0749\n","Epoch 48: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 68ms/step - loss: 0.0749 - val_loss: 0.5794\n","Epoch 49/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0922\n","Epoch 49: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0922 - val_loss: 0.6003\n","Epoch 50/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1039\n","Epoch 50: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 68ms/step - loss: 0.1039 - val_loss: 0.6104\n","Epoch 51/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1085\n","Epoch 51: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 102ms/step - loss: 0.1085 - val_loss: 0.6104\n","Epoch 52/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.1061\n","Epoch 52: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 48ms/step - loss: 0.1061 - val_loss: 0.6015\n","Epoch 53/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0973\n","Epoch 53: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0973 - val_loss: 0.5848\n","Epoch 54/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0837\n","Epoch 54: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0837 - val_loss: 0.5622\n","Epoch 55/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0673\n","Epoch 55: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0673 - val_loss: 0.5351\n","Epoch 56/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0502\n","Epoch 56: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 47ms/step - loss: 0.0502 - val_loss: 0.5053\n","Epoch 57/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0344\n","Epoch 57: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 49ms/step - loss: 0.0344 - val_loss: 0.4747\n","Epoch 58/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0217\n","Epoch 58: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0217 - val_loss: 0.4448\n","Epoch 59/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0133\n","Epoch 59: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 78ms/step - loss: 0.0133 - val_loss: 0.4171\n","Epoch 60/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0096\n","Epoch 60: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0096 - val_loss: 0.3928\n","Epoch 61/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0101\n","Epoch 61: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 55ms/step - loss: 0.0101 - val_loss: 0.3726\n","Epoch 62/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0136\n","Epoch 62: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 71ms/step - loss: 0.0136 - val_loss: 0.3571\n","Epoch 63/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0184\n","Epoch 63: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 73ms/step - loss: 0.0184 - val_loss: 0.3464\n","Epoch 64/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0229\n","Epoch 64: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0229 - val_loss: 0.3403\n","Epoch 65/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0257\n","Epoch 65: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0257 - val_loss: 0.3386\n","Epoch 66/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0261\n","Epoch 66: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 53ms/step - loss: 0.0261 - val_loss: 0.3409\n","Epoch 67/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0240\n","Epoch 67: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 54ms/step - loss: 0.0240 - val_loss: 0.3466\n","Epoch 68/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0201\n","Epoch 68: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0201 - val_loss: 0.3552\n","Epoch 69/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0153\n","Epoch 69: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 51ms/step - loss: 0.0153 - val_loss: 0.3661\n","Epoch 70/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0104\n","Epoch 70: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0104 - val_loss: 0.3784\n","Epoch 71/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0064\n","Epoch 71: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 88ms/step - loss: 0.0064 - val_loss: 0.3914\n","Epoch 72/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0036\n","Epoch 72: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 82ms/step - loss: 0.0036 - val_loss: 0.4043\n","Epoch 73/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0022\n","Epoch 73: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 52ms/step - loss: 0.0022 - val_loss: 0.4164\n","Epoch 74/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0019\n","Epoch 74: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 72ms/step - loss: 0.0019 - val_loss: 0.4272\n","Epoch 75/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0024\n","Epoch 75: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 74ms/step - loss: 0.0024 - val_loss: 0.4360\n","Epoch 76/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0034\n","Epoch 76: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 76ms/step - loss: 0.0034 - val_loss: 0.4428\n","Epoch 77/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0044\n","Epoch 77: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 69ms/step - loss: 0.0044 - val_loss: 0.4472\n","Epoch 78/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0052\n","Epoch 78: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 57ms/step - loss: 0.0052 - val_loss: 0.4493\n","Epoch 79/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0056\n","Epoch 79: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 56ms/step - loss: 0.0056 - val_loss: 0.4492\n","Epoch 80/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0055\n","Epoch 80: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 37ms/step - loss: 0.0055 - val_loss: 0.4471\n","Epoch 81/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0049\n","Epoch 81: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0049 - val_loss: 0.4433\n","Epoch 82/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0041\n","Epoch 82: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0041 - val_loss: 0.4382\n","Epoch 83/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0031\n","Epoch 83: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0031 - val_loss: 0.4322\n","Epoch 84/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0021\n","Epoch 84: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 45ms/step - loss: 0.0021 - val_loss: 0.4257\n","Epoch 85/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0013\n","Epoch 85: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 40ms/step - loss: 0.0013 - val_loss: 0.4191\n","Epoch 86/3000\n","1/1 [==============================] - ETA: 0s - loss: 7.1558e-04\n","Epoch 86: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 39ms/step - loss: 7.1558e-04 - val_loss: 0.4128\n","Epoch 87/3000\n","1/1 [==============================] - ETA: 0s - loss: 4.1347e-04\n","Epoch 87: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 38ms/step - loss: 4.1347e-04 - val_loss: 0.4070\n","Epoch 88/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.6943e-04\n","Epoch 88: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 40ms/step - loss: 3.6943e-04 - val_loss: 0.4020\n","Epoch 89/3000\n","1/1 [==============================] - ETA: 0s - loss: 5.1864e-04\n","Epoch 89: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 45ms/step - loss: 5.1864e-04 - val_loss: 0.3979\n","Epoch 90/3000\n","1/1 [==============================] - ETA: 0s - loss: 7.7074e-04\n","Epoch 90: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 43ms/step - loss: 7.7074e-04 - val_loss: 0.3950\n","Epoch 91/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0010\n","Epoch 91: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0010 - val_loss: 0.3933\n","Epoch 92/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0012\n","Epoch 92: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 44ms/step - loss: 0.0012 - val_loss: 0.3926\n","Epoch 93/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0013\n","Epoch 93: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 48ms/step - loss: 0.0013 - val_loss: 0.3930\n","Epoch 94/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0013\n","Epoch 94: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 39ms/step - loss: 0.0013 - val_loss: 0.3944\n","Epoch 95/3000\n","1/1 [==============================] - ETA: 0s - loss: 0.0011\n","Epoch 95: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 43ms/step - loss: 0.0011 - val_loss: 0.3966\n","Epoch 96/3000\n","1/1 [==============================] - ETA: 0s - loss: 8.6938e-04\n","Epoch 96: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 43ms/step - loss: 8.6938e-04 - val_loss: 0.3994\n","Epoch 97/3000\n","1/1 [==============================] - ETA: 0s - loss: 6.1649e-04\n","Epoch 97: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 41ms/step - loss: 6.1649e-04 - val_loss: 0.4026\n","Epoch 98/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.8277e-04\n","Epoch 98: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 41ms/step - loss: 3.8277e-04 - val_loss: 0.4060\n","Epoch 99/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.0101e-04\n","Epoch 99: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 43ms/step - loss: 2.0101e-04 - val_loss: 0.4094\n","Epoch 100/3000\n","1/1 [==============================] - ETA: 0s - loss: 8.8194e-05\n","Epoch 100: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 50ms/step - loss: 8.8194e-05 - val_loss: 0.4127\n","Epoch 101/3000\n","1/1 [==============================] - ETA: 0s - loss: 4.5231e-05\n","Epoch 101: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 45ms/step - loss: 4.5231e-05 - val_loss: 0.4157\n","Epoch 102/3000\n","1/1 [==============================] - ETA: 0s - loss: 5.9928e-05\n","Epoch 102: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 43ms/step - loss: 5.9928e-05 - val_loss: 0.4183\n","Epoch 103/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.1200e-04\n","Epoch 103: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 45ms/step - loss: 1.1200e-04 - val_loss: 0.4204\n","Epoch 104/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.7858e-04\n","Epoch 104: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 47ms/step - loss: 1.7858e-04 - val_loss: 0.4219\n","Epoch 105/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.3914e-04\n","Epoch 105: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 38ms/step - loss: 2.3914e-04 - val_loss: 0.4228\n","Epoch 106/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.7886e-04\n","Epoch 106: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 40ms/step - loss: 2.7886e-04 - val_loss: 0.4232\n","Epoch 107/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.9035e-04\n","Epoch 107: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 41ms/step - loss: 2.9035e-04 - val_loss: 0.4230\n","Epoch 108/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.7354e-04\n","Epoch 108: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 47ms/step - loss: 2.7354e-04 - val_loss: 0.4224\n","Epoch 109/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.3423e-04\n","Epoch 109: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 48ms/step - loss: 2.3423e-04 - val_loss: 0.4214\n","Epoch 110/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.8187e-04\n","Epoch 110: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 42ms/step - loss: 1.8187e-04 - val_loss: 0.4202\n","Epoch 111/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.2698e-04\n","Epoch 111: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 44ms/step - loss: 1.2698e-04 - val_loss: 0.4187\n","Epoch 112/3000\n","1/1 [==============================] - ETA: 0s - loss: 7.8866e-05\n","Epoch 112: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 38ms/step - loss: 7.8866e-05 - val_loss: 0.4172\n","Epoch 113/3000\n","1/1 [==============================] - ETA: 0s - loss: 4.4018e-05\n","Epoch 113: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 42ms/step - loss: 4.4018e-05 - val_loss: 0.4157\n","Epoch 114/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.5277e-05\n","Epoch 114: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 44ms/step - loss: 2.5277e-05 - val_loss: 0.4142\n","Epoch 115/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.1912e-05\n","Epoch 115: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 44ms/step - loss: 2.1912e-05 - val_loss: 0.4130\n","Epoch 116/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.0374e-05\n","Epoch 116: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 44ms/step - loss: 3.0374e-05 - val_loss: 0.4119\n","Epoch 117/3000\n","1/1 [==============================] - ETA: 0s - loss: 4.5497e-05\n","Epoch 117: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 46ms/step - loss: 4.5497e-05 - val_loss: 0.4111\n","Epoch 118/3000\n","1/1 [==============================] - ETA: 0s - loss: 6.1815e-05\n","Epoch 118: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 40ms/step - loss: 6.1815e-05 - val_loss: 0.4106\n","Epoch 119/3000\n","1/1 [==============================] - ETA: 0s - loss: 7.4715e-05\n","Epoch 119: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 41ms/step - loss: 7.4715e-05 - val_loss: 0.4103\n","Epoch 120/3000\n","1/1 [==============================] - ETA: 0s - loss: 8.1211e-05\n","Epoch 120: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 40ms/step - loss: 8.1211e-05 - val_loss: 0.4103\n","Epoch 121/3000\n","1/1 [==============================] - ETA: 0s - loss: 8.0251e-05\n","Epoch 121: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 41ms/step - loss: 8.0251e-05 - val_loss: 0.4105\n","Epoch 122/3000\n","1/1 [==============================] - ETA: 0s - loss: 7.2569e-05\n","Epoch 122: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 49ms/step - loss: 7.2569e-05 - val_loss: 0.4109\n","Epoch 123/3000\n","1/1 [==============================] - ETA: 0s - loss: 6.0203e-05\n","Epoch 123: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 41ms/step - loss: 6.0203e-05 - val_loss: 0.4115\n","Epoch 124/3000\n","1/1 [==============================] - ETA: 0s - loss: 4.5824e-05\n","Epoch 124: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 40ms/step - loss: 4.5824e-05 - val_loss: 0.4122\n","Epoch 125/3000\n","1/1 [==============================] - ETA: 0s - loss: 3.2077e-05\n","Epoch 125: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 53ms/step - loss: 3.2077e-05 - val_loss: 0.4129\n","Epoch 126/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.1035e-05\n","Epoch 126: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 45ms/step - loss: 2.1035e-05 - val_loss: 0.4137\n","Epoch 127/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.3885e-05\n","Epoch 127: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 41ms/step - loss: 1.3885e-05 - val_loss: 0.4144\n","Epoch 128/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.0849e-05\n","Epoch 128: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 42ms/step - loss: 1.0849e-05 - val_loss: 0.4151\n","Epoch 129/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.1324e-05\n","Epoch 129: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 44ms/step - loss: 1.1324e-05 - val_loss: 0.4157\n","Epoch 130/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.4155e-05\n","Epoch 130: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 45ms/step - loss: 1.4155e-05 - val_loss: 0.4161\n","Epoch 131/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.7969e-05\n","Epoch 131: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 45ms/step - loss: 1.7969e-05 - val_loss: 0.4164\n","Epoch 132/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.1495e-05\n","Epoch 132: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 49ms/step - loss: 2.1495e-05 - val_loss: 0.4166\n","Epoch 133/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.3792e-05\n","Epoch 133: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 46ms/step - loss: 2.3792e-05 - val_loss: 0.4166\n","Epoch 134/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.4379e-05\n","Epoch 134: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 40ms/step - loss: 2.4379e-05 - val_loss: 0.4165\n","Epoch 135/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.3236e-05\n","Epoch 135: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 43ms/step - loss: 2.3236e-05 - val_loss: 0.4163\n","Epoch 136/3000\n","1/1 [==============================] - ETA: 0s - loss: 2.0724e-05\n","Epoch 136: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 41ms/step - loss: 2.0724e-05 - val_loss: 0.4160\n","Epoch 137/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.7439e-05\n","Epoch 137: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 43ms/step - loss: 1.7439e-05 - val_loss: 0.4157\n","Epoch 138/3000\n","1/1 [==============================] - ETA: 0s - loss: 1.4036e-05\n","Epoch 138: val_loss did not improve from 0.18577\n","1/1 [==============================] - 0s 42ms/step - loss: 1.4036e-05 - val_loss: 0.4153\n","1/1 [==============================] - 0s 116ms/step - loss: 0.2253\n","loss_and_metrics : 0.2253251075744629\n","1/1 [==============================] - 0s 107ms/step\n"]}],"source":["import scipy\n","import numpy\n","import h5py\n","\n","#import tensorflow\n","from tensorflow import keras\n","\n","#print('scipy ' + scipy.__version__)\n","#print('numpy ' + numpy.__version__)\n","#print('h5py ' + h5py.__version__)\n","\n","#print('tensorflow ' + tensorflow.__version__)\n","#print('keras ' + keras.__version__)\n","\n","import scipy.io\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Dropout, Activation\n","#from keras.optimizers import SGD\n","from tensorflow.keras.optimizers import Adam\n","#from keras.optimizers import Nadam\n","#from keras.optimizers import RMSprop\n","from tensorflow.keras.optimizers import Adamax\n","from tensorflow.keras.datasets import cifar10\n","#error발생: from tensorflow.keras.utils import np_utils\n","from tensorflow.keras.utils import to_categorical\n","\n","\n","train_x_data = scipy.io.loadmat('ml_detect_in_train.mat')\n","train_y_data = scipy.io.loadmat('ml_detect_out_train.mat')\n","\n","train_x = train_x_data['in']\n","train_y = train_y_data['out']\n","\n","\n","\n","val_x_data = scipy.io.loadmat('ml_detect_in_val.mat')\n","val_y_data = scipy.io.loadmat('ml_detect_out_val.mat')\n","\n","val_x = val_x_data['in']\n","val_y = val_y_data['out']\n","\n","\n","# relu, tanh, elu, selu\n","\n","model = Sequential()\n","model.add(Dense(units=100, input_dim=40, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=100, activation=\"relu\", kernel_initializer=\"normal\"))\n","#model.add(Dropout(0.5))\n","model.add(Dense(units=4, activation=\"linear\", kernel_initializer='normal'))\n","\n","\n","#model.compile(loss='mean_squared_error', optimizer='adam')\n","model.compile(loss='mean_squared_error', optimizer='adamax')\n","\n","#model.fit(train_x, train_y, epochs=1000, batch_size=32)\n","\n","from keras.callbacks import EarlyStopping\n","from keras.callbacks import ModelCheckpoint\n","early_stopping = EarlyStopping(patience = 100) # 조기종료 콜백함수 정의, 100 에포크 동안은 기다림\n","checkpoint_callback = ModelCheckpoint('hl5_0100.h5', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n","model.fit(train_x, train_y, epochs=3000, batch_size=32, validation_data=(val_x, val_y), callbacks=[early_stopping, checkpoint_callback])\n","\n","\n","from keras.models import load_model\n","model_cp = load_model('hl5_0100.h5')\n","\n","test_x_data = scipy.io.loadmat('ml_detect_in_test.mat')\n","test_y_data = scipy.io.loadmat('ml_detect_out_test.mat')\n","test_x = test_x_data['in']\n","test_y = test_y_data['out']\n","\n","loss_and_metrics = model_cp.evaluate(test_x, test_y, batch_size=32)\n","\n","print('loss_and_metrics : ' + str(loss_and_metrics))\n","\n","\n","yhat=model_cp.predict(test_x)\n","scipy.io.savemat('hl5_0500_pred.mat',dict([('predict_ch', yhat) ]))"]}]}